from urllib.request import urlopen
from bs4 import BeautifulSoup as bs
from datetime import datetime

def fetch_kyobo_best_sellers():
    url = "http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf"
    html = urlopen(url)
    bsObject = bs(html, "html.parser")

    # ì§‘ê³„ ê¸°ì¤€ ë‚ ì§œ ì¶”ì¶œ
    try:
        week_standard = bsObject.find('h4', {'class': 'title_best_basic'}).find('small').text.strip()
    except AttributeError:
        week_standard = "(ë‚ ì§œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"

    # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    bestseller_contents = bsObject.find('ul', {'class': 'list_type01'})
    bestseller_list = bestseller_contents.findAll('div', {'class': 'detail'})

    books = []
    for idx, book in enumerate(bestseller_list[:20], start=1):
        title = book.find('div', {'class': 'title'}).find('strong').text.strip()
        subtitle = book.find('div', {'class': 'subtitle'}).text.strip()
        books.append(f"{idx}. **{title}**\n   - {subtitle}")

    return week_standard, books

def update_readme(week_standard, books):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    content = f"# ğŸ“š êµë³´ë¬¸ê³  ë² ìŠ¤íŠ¸ì…€ëŸ¬ (ì—…ë°ì´íŠ¸: {now})\n\n"
    content += f"ğŸ“… ê¸°ì¤€ì¼: {week_standard}\n\n"

    if books:
        content += "\n".join(books)
    else:
        content += "â—ï¸ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    content += "\n\nìë™ ì—…ë°ì´íŠ¸: GitHub Actions ì‚¬ìš©"

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)

if __name__ == "__main__":
    week_standard, best_sellers = fetch_kyobo_best_sellers()
    update_readme(week_standard, best_sellers)
